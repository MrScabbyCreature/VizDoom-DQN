{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vizdoom as vzd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "from collections import deque\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "config_file = \"defend_the_center.cfg\"\n",
    "\n",
    "#dqn parameters\n",
    "l_r = 0.001\n",
    "batch_size = 50\n",
    "episodes_per_epoch = 100\n",
    "epochs = 50\n",
    "\n",
    "# input parameters\n",
    "input_shape = (100, 100)\n",
    "replay_buffer_size = 500\n",
    "\n",
    "#game parameters\n",
    "frames_to_skip = 10\n",
    "discount = 0.9\n",
    "\n",
    "# helper functions\n",
    "def display_image(image):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "def process_raw_input(image):\n",
    "    return cv2.resize(image, input_shape).reshape((*input_shape, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize doom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_game(visible=False):\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(\"../../scenarios/\" + config_file)\n",
    "    game.set_window_visible(visible)\n",
    "    game.set_mode(vzd.Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_1280X1024)\n",
    "    game.init()\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    def __init__(self, num_actions):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default(): \n",
    "            self._X = tf.placeholder(tf.float32, [None] + list(input_shape) + [1], name=\"X\")\n",
    "    #         a_ = tf.placeholder(tf.int32, [None], name=\"Action\")\n",
    "            self._y = tf.placeholder(tf.float32, [None, num_actions], name=\"y_true\")\n",
    "\n",
    "            # convolutional layers\n",
    "            self._conv1 = tf.layers.conv2d(\n",
    "                                inputs=self._X,\n",
    "                                filters=8,\n",
    "                                kernel_size=[9, 9],\n",
    "                                strides=[2, 2],\n",
    "                                activation=tf.nn.relu,\n",
    "                                name=\"conv1\")\n",
    "            self._pool1 = tf.layers.max_pooling2d(inputs=self._conv1, pool_size=[2, 2], strides=2, name=\"pool1\")\n",
    "\n",
    "            self._conv2 = tf.layers.conv2d(\n",
    "                                inputs=self._pool1,\n",
    "                                filters=32,\n",
    "                                kernel_size=[5, 5],\n",
    "                                strides=[2, 2],\n",
    "                                activation=tf.nn.relu,\n",
    "                                name=\"conv2\", )\n",
    "            self._pool2 = tf.layers.max_pooling2d(inputs=self._conv2, pool_size=[2, 2], strides=2, name=\"pool2\")\n",
    "\n",
    "            #fcn\n",
    "            self._flat = tf.layers.flatten(self._pool2, name=\"flat\")\n",
    "            self._fc1 = tf.layers.dense(inputs=self._flat, units=512, activation=tf.nn.relu, name=\"fc1\", )\n",
    "            self._dropout = tf.layers.dropout(inputs=self._fc1, rate=0.5, name=\"dropout\")\n",
    "\n",
    "            self._y_pred = tf.layers.dense(inputs=self._dropout, units=num_actions, name=\"y_pred\", )\n",
    "\n",
    "            #train\n",
    "            self._loss = tf.losses.mean_squared_error(labels=self._y, predictions=self._y_pred)\n",
    "            self._optimizer = tf.train.AdamOptimizer(learning_rate=l_r).minimize(self._loss)\n",
    "            self._session = tf.Session(graph=self.graph)\n",
    "            self._session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(self, X, y):\n",
    "        with self.graph.as_default():\n",
    "            loss, _ = self._session.run([self._loss, self._optimizer], feed_dict={self._X: X, self._y: y})\n",
    "        return loss\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        return self._session.run(self._y_pred, feed_dict={self._X: state})\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        return self.get_q_values(state).argmax()\n",
    "    \n",
    "    def finish(self):\n",
    "        self._session.close()\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(self._session, path)\n",
    "            \n",
    "    def restore_model(self, path):\n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self._session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LET'S PLAY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to experiment with how many actions to allow to occur simultaneously\n",
    "def create_actions(listy, n, max_ones=1):\n",
    "    '''\n",
    "    returns a list of all combinations of 0's and 1's of length n with max number of 1's=max_ones\n",
    "    '''\n",
    "    if sum(listy) >= max_ones:\n",
    "        if len(listy) < n:\n",
    "            listy += [0]*(n-len(listy))\n",
    "        return listy\n",
    "    if len(listy) >= n:\n",
    "        return listy\n",
    "    return put_it_in_a_single_list([create_actions(listy + [0], n, max_ones), create_actions(listy + [1], n, max_ones)])\n",
    "\n",
    "def put_it_in_a_single_list(listy):\n",
    "    result = []\n",
    "    def recurse(listy):\n",
    "        for obj in listy:\n",
    "            if type(obj) != list:\n",
    "                result.append(listy)\n",
    "                break\n",
    "            else:\n",
    "                recurse(obj)\n",
    "    recurse(listy)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/main_save\n",
      "Actions:  [[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "model = DQN(4)\n",
    "model.restore_model(\"model/main_save\")\n",
    "game = initialize_game()\n",
    "#actions\n",
    "n = game.get_available_buttons_size()\n",
    "actions = create_actions([], n, 1) #change arg3 to range(1, n)\n",
    "print(\"Actions: \", actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to see our model playing the game.\n",
    "\n",
    "def play_optimal(visible=False):\n",
    "    import time\n",
    "    game = initialize_game(visible)\n",
    "    game.new_episode()\n",
    "    while not game.is_episode_finished():\n",
    "        state = process_raw_input(game.get_state().screen_buffer)\n",
    "        game.make_action(actions[model.get_best_action(state.reshape((1, *state.shape)))], frames_to_skip)\n",
    "        if visible:\n",
    "            time.sleep(0.2)\n",
    "    game.set_window_visible(False)\n",
    "    return game.get_total_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for 1 episodes: 3.0\n",
      "Average reward for 11 episodes: 4.454545454545454\n",
      "Average reward for 21 episodes: 4.523809523809524\n",
      "Average reward for 31 episodes: 4.967741935483871\n",
      "Average reward for 41 episodes: 4.926829268292683\n",
      "Average reward for 51 episodes: 4.745098039215686\n",
      "Average reward for 61 episodes: 4.754098360655738\n",
      "Average reward for 71 episodes: 4.619718309859155\n",
      "Average reward for 81 episodes: 4.617283950617284\n",
      "Average reward for 91 episodes: 4.725274725274725\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.860678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  100.000000\n",
       "mean     4.750000\n",
       "std      1.860678\n",
       "min      1.000000\n",
       "25%      3.000000\n",
       "50%      4.500000\n",
       "75%      6.000000\n",
       "max     13.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = 100\n",
    "reward_list = []\n",
    "for epi in range(episodes):\n",
    "    reward_list.append(play_optimal(visible=False))\n",
    "    if epi % 10 == 0:\n",
    "        print(\"Average reward for {} episodes: {}\".format(epi+1, sum(reward_list)/(epi+1)))\n",
    "\n",
    "pd.DataFrame(reward_list).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
